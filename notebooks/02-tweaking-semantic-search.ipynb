{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5527397e89ce8f7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tweaking up semantic retrieval\n",
    "\n",
    "There are various objectives we could try optimizing for when it comes to semantic retrieval. We could try to optimize the **speed** of the retrieval, the **quality** of it, or the **memory usage**. We'll review some of the techniques in all three areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97156bfd207c831",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading the configuration and pipeline\n",
    "\n",
    "Again, let's start with loading the configuration, and then set up our retriever. We don't want a full RAG pipeline, as we are solely interested in the semantic search part. Improving a single component at a time should be easier to understand and debug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5400644c6fa94d96",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c571d2c60524ef3d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=\"local:BAAI/bge-large-en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480e88c42e30d3cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    # api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"hacker-news\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb26026d9b2d8e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44471047edccc2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MetadataFilters, MetadataFilter\n",
    "from llama_index.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(key=\"type\", value=\"story\"),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fefadc69369516",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Ask HN: Where do you go to find recommendations for physical programming books?\n",
      "\n",
      "I&#x27;m old school and like sitting down with a book both for learning actual coding and also for methodologies and philosophies. I don&#x27;t know where to go for recommendations. Any help? Thanks!\n",
      "\n",
      "2 Ask HN: What to learn in order to get a software job in a decent country?\n",
      "\n",
      "A good friend of mine is 18 and Russian. He is a programming prodigy and is trying to formulate a plan to get out. He&#x27;s thinking about his future CV and applying for jobs. What would be the best frameworks to invest time in getting experience with now?\n",
      "\n",
      "3 Ask HN: What is the best way to get into building electronics as a programmer?\n",
      "\n",
      "I am asking not only about learning what is taught in classes for solving ideal problems. I am talking about the real engineering like a hobbyist who actually understands what works in real life and how to build it properly.\n",
      "\n",
      "4 Ask HN: Best tools for 4/5 year old to learn programming?\n",
      "\n",
      "I&#x27;m looking for the best systems to help a 4&#x2F;5 year old get the basics of programming. My daughter has shown interest in what I do, and loves puzzles and building things. Looking for something visual and fun that can start her down the path of logic and creating with computers.<p>I have a passing familiarity with Scratch [1], which I&#x27;m now looking into more, but am hoping others can share their knowledge and experience in this area.<p>[1] https:&#x2F;&#x2F;scratch.mit.edu&#x2F;\n",
      "\n",
      "5 Ask HN: What is the best way to learn a new language?\n",
      "\n",
      "Is using a flashcard app like Anki the best way? Duolingo? Memrise? etc<p>For context, I was thinking of learning either Spanish, Dutch or Arabic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8084e400f1ff37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Quality optimization\n",
    "\n",
    "We have implemented a basic RAG already, and we might be happy with the quality. There are a lot of aspects when it comes to measuring the quality of a semantic retrieval system, and we will not go into details here. It is usually related to the quality of the embedding model we use, and it is a topic for another day.\n",
    "\n",
    "However, all the vector databases approximate the nearest neighbor search, and this approximation comes with a cost. The cost is that the results are not always ideal. HNSW, an algorithm used in Qdrant, has some parameters to control how the internal structures are built, and these parameters can be tweaked to improve the quality of the results. This is very specific to the vector database used, thus it's configured through the Qdrant API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18105b7cfb654cb6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=47037, indexed_vectors_count=45000, points_count=47037, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={'type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=47037)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collection(collection_name=\"hacker-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7eb35651edebd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As for now, the most interesting part is the `hnsw_config` field. The algorithm itself is controlled by two parameters. The number of edges per node is called the `m` parameter. The larger the value, the higher the precision of the search, but the more space required. The `ef_construct` parameter is the number of neighbors to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time. \n",
    "\n",
    "Playing with both parameters **improves just the approximation of the exact nearest neighbors**, and a proper embedding model is still way more important. However, [this quality aspect might also be controlled, even in an automated way](https://qdrant.tech/documentation/tutorials/retrieval-quality/). For the time being, we'll simply increase both values, but won't measure the impact on the overall quality of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3668e7f04e38d00f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    hnsw_config=models.HnswConfigDiff(\n",
    "        m=32,\n",
    "        ef_construct=200,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4d08884025a701",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=47037, indexed_vectors_count=45000, points_count=47037, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=32, ef_construct=200, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={'type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=47037)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2ede02bc5281ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Ask HN: What would you look for in a platform to learn programming?\n",
      "\n",
      "Hey everyone!<p>I&#x27;m curious, what does the perfect programming education platform look like to you?<p>I&#x27;m an experienced developer, but I really think that the current options for learning programming could be a lot better. I know that there are platforms like CodeCademy and places to watch video courses like YouTube and Udemy. There are also so many scammy &quot;learn to code&quot; sites (CodeFinity).<p>The pattern I notice is that platforms like CodeCademy are web-apps and are very career-path-oriented (i.e. get certifications). I personally think that having a platform which was a desktop app would be a better solution. Rather than focusing on career-tracks, you could follow courses to build a specific project using an integrated IDE, 100% on your machine.<p>How important are career-tracks and certifications to you? Or, would you rather just learn to build a specific project on your own machine all in one application?<p>Basically, a native desktop platform to learn programming by actually building projects on your own machine. No BS.\n",
      "\n",
      "2 Ask HN: Resources to learn programming from ground up?\n",
      "\n",
      "Typically learning programming involves learning syntax, patterns and algorithms. Are there resources to learn it from ground up ? For instance why C is structured the way it is and how cpu and memory can be optimized by the program etc ?\n",
      "\n",
      "3 Ask HN: Where do you go to find recommendations for physical programming books?\n",
      "\n",
      "I&#x27;m old school and like sitting down with a book both for learning actual coding and also for methodologies and philosophies. I don&#x27;t know where to go for recommendations. Any help? Thanks!\n",
      "\n",
      "4 Ask HN: What are the best jobs to help me learn how to code on the side?\n",
      "\n",
      "Hello,<p>I had a question, partly inspired by a post someone made here a few years ago about what&#x27;s the fastest way to become employable in tech.<p>I didn&#x27;t study CS and am beginning to learn about programming in the evenings as my current job (Legal sector, non-technical) is unrelated.<p>At the moment I&#x27;m still just learning and doing it for fun but it&#x27;d be nice if there was a potential path to doing it for money&#x2F;being employed one day, if I still enjoy it.<p>I know obviously any job would work, but are there specific types of jobs you&#x27;d recommend that are more &quot;tech-adjacent&quot; so I could, to some extent, learn on the job?<p>An example someone used in the previous thread was trying to work in tech support, like an entry job in quality assurance&#x2F;control (QA&#x2F;QC), or a tech support role at a small&#x2F;mid-sized startup. (was also curious if you&#x27;d heavily priortise in-person roles to learn from colleagues, or whether it doesn&#x27;t matter and hybrid&#x2F;remote roles would suffice - I don&#x27;t have a preference for the right job, I&#x27;m UK-based btw)<p>I was just curious if there are options where I could work around&#x2F;with software developers and be exposed to more technology during my day job, to then help with learning programming in the evenings&#x2F;weekends. Or, would you just recommend doing a bootcamp?<p>I know this journey is for the long haul, but was curious if there are any types of jobs I should be targeting: where I can get good exposure to programming and tech in general, without having the experience.\n",
      "\n",
      "5 Ask HN: What to learn in order to get a software job in a decent country?\n",
      "\n",
      "A good friend of mine is 18 and Russian. He is a programming prodigy and is trying to formulate a plan to get out. He&#x27;s thinking about his future CV and applying for jobs. What would be the best frameworks to invest time in getting experience with now?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b5c8fd6513bb5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Memory optimization\n",
    "\n",
    "Each point in a Qdrant collection consists of up to three elements: id, vector(s), and optional payload represented by a JSON object. Vectors are indexed in an HNSW graph, and search operations may involve semantic similarity and some payload-based criteria (it's best to add payload indexes on the fields we want to use for the filtering). Ideally, all the elements should be kept in RAM so access is fast.\n",
    "\n",
    "Unfortunately, semantic search is a heavy operation in terms of memory requirements. However, some projects are implemented on a budget and can't afford machines with hundreds of gigabytes of RAM. Qdrant allows storing every single component on a disk to reduce memory usage, but that comes with a performance cost. Let's compare the efficiency of the operations with all the components in RAM and with some of them on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d2159e4871b1dd6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.3 ms ± 2.96 ms per loop (mean ± std. dev. of 5 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1534ffc86ce1f7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    hnsw_config=models.HnswConfigDiff(\n",
    "        on_disk=True,\n",
    "    ),\n",
    "    vectors_config={\n",
    "        \"\": models.VectorParamsDiff(\n",
    "            on_disk=True,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396074dc50565c7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=47037, indexed_vectors_count=47037, points_count=47037, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=True), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=32, ef_construct=200, full_scan_threshold=10000, max_indexing_threads=0, on_disk=True, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={'type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=47037)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f500d12db05fdc6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.7 ms ± 1.18 ms per loop (mean ± std. dev. of 5 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9fc24e922f165",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Speed optimization\n",
    "\n",
    "There are various ways of optimizing semantic search in terms of speed. The most straightforward one is to reduce both `m` and `ef_construct` parameters, as we did in the previous section. However, this comes with a cost of the quality of the results.\n",
    "\n",
    "Qdrant also provides a number of quantization techniques, and two of them are primarily used to increase speed and reduce memory at the same time:\n",
    "\n",
    "1. **Scalar Quantization** - uses `int8` instead of `float32` to store each vector dimension\n",
    "2. **Binary Quantization** - `bool` values are used to store each vector dimension\n",
    "\n",
    "The first one reduces the memory usage by up to 4x, while the second one by up to 32x and both increase the speed of the search. However, the quality of the search results is reduced, and Binary Quantization is not suitable for all the use cases. It only works with some specific models, usually the ones with high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1154f4172094cdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In our case, we're going to set up the binary quantization either way. From the LlamaIndex perspective, the search operations are going to be fired identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e26ec00449bfb27",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    quantization_config=models.BinaryQuantization(\n",
    "        binary=models.BinaryQuantizationConfig(\n",
    "            always_ram=True,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a01cc56dff3ea252",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=47037, indexed_vectors_count=47037, points_count=47037, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=True), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=32, ef_construct=200, full_scan_threshold=10000, max_indexing_threads=0, on_disk=True, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True))), payload_schema={'type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=47037)})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4d07f587535769",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Ask HN: What would you look for in a platform to learn programming?\n",
      "\n",
      "Hey everyone!<p>I&#x27;m curious, what does the perfect programming education platform look like to you?<p>I&#x27;m an experienced developer, but I really think that the current options for learning programming could be a lot better. I know that there are platforms like CodeCademy and places to watch video courses like YouTube and Udemy. There are also so many scammy &quot;learn to code&quot; sites (CodeFinity).<p>The pattern I notice is that platforms like CodeCademy are web-apps and are very career-path-oriented (i.e. get certifications). I personally think that having a platform which was a desktop app would be a better solution. Rather than focusing on career-tracks, you could follow courses to build a specific project using an integrated IDE, 100% on your machine.<p>How important are career-tracks and certifications to you? Or, would you rather just learn to build a specific project on your own machine all in one application?<p>Basically, a native desktop platform to learn programming by actually building projects on your own machine. No BS.\n",
      "\n",
      "2 Ask HN: What is the best way to learn a new language?\n",
      "\n",
      "Is using a flashcard app like Anki the best way? Duolingo? Memrise? etc<p>For context, I was thinking of learning either Spanish, Dutch or Arabic\n",
      "\n",
      "3 Ask HN: Resources for kids/teens to learn programming\n",
      "\n",
      "My friend is seeking online courses or resources for his 12-year-old child to learn programming skills.\n",
      "\n",
      "4 Ask HN: Learning Swift as an Existing Programmer?\n",
      "\n",
      "It&#x27;s been a while since I&#x27;ve had to learn a new language &amp; stack from scratch. What are some good resources for learning Swift as an existing developer? There are a ton of courses online for non-programmers, but I don&#x27;t want to learn what a print statement or conditional is. Instead, I want to learn how modern UI toolkits are used, how network calls are handled, best practices for async, multi-threading, error handling, etc. Real business logic stuff. Are there any courses like that, free or paid?<p>I&#x27;m a web dev, mostly frontend, who just wants to make a macOS desktop app for fun. I know most of app development is on iOS these days, but I don&#x27;t particularly like that platform and would rather focus on the desktop, if there&#x27;s a difference. I&#x27;ve previously learned React, JS, PHP, Perl, and Visual Basic.<p>Not really interested in React Native or other similar JS-to-native frameworks. I want to make a lean, fast, simple and unbloated app with native look &amp; feel.\n",
      "\n",
      "5 Ask HN: What's your experience of learning to type?\n",
      "\n",
      "Bit of a shower thoughts situation... I was recalling what it was like as a kid to learn to write using a pencil&#x2F;pen; these memories are relatively vivid. I realised that I can&#x27;t recall the same about learning to type.<p>I figure I would recall being intrigued by the keyboard&#x27;s layout, the feeling and function of it, and recall finding it cumbersome at first and then finding it progressively easier. But no, I feel like I&#x27;ve just... always known?<p>So I&#x27;m curious to know how others experience this.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "408270c8449987c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6 ms ± 731 µs per loop (mean ± std. dev. of 5 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dbd5e38b33efd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
